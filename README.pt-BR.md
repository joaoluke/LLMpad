# LLMpad

Um aplicativo de desktop leve para conversar com LLMs locais, constru√≠do com Tauri + React.

## Funcionalidades

- ü™∂ **Leve** - Usa Tauri (Rust) em vez de Electron
- üíæ **Persist√™ncia** - Conversas salvas em SQLite local
- üîå **Compat√≠vel** - Funciona com qualquer API compat√≠vel com OpenAI (Ollama, LM Studio, etc.)
- üñ•Ô∏è **Multiplataforma** - Mac, Linux e Windows
- ü§ñ **Gerenciador de Modelos** - Crie e gerencie modelos personalizados do Ollama

## Requisitos

- [Node.js](https://nodejs.org/) (v18+)
- [Rust](https://www.rust-lang.org/tools/install)
- [Ollama](https://ollama.ai/) (recomendado para execu√ß√£o local)

## Instala√ß√£o

```bash
# Instalar depend√™ncias
npm install

# Executar em modo de desenvolvimento
npm run tauri dev

# Construir para produ√ß√£o
npm run tauri build
```

## Configura√ß√£o

1. Abra o aplicativo
2. Clique em "Configura√ß√µes" (‚öôÔ∏è)
3. Configure a URL da API:
   - **Ollama**: `http://localhost:11434/v1`
   - **LM Studio**: `http://localhost:1234/v1`
   - **OpenAI**: `https://api.openai.com/v1`
4. Selecione o modelo na lista suspensa
5. Chave de API √© opcional para APIs locais

## Gerenciando Modelos Personalizados

### Criando um novo modelo

1. Crie um arquivo `.Modelfile` na pasta `models/`
2. Exemplo (`models/meu-modelo.Modelfile`):
   ```
   FROM phi3:latest
   
   PARAMETER temperature 0.7
   PARAMETER num_ctx 2048
   
   SYSTEM """
   Voc√™ √© um assistente √∫til e prestativo.
   """
   ```
3. No aplicativo, v√° para Configura√ß√µes > Gerenciar Modelos
4. Clique em "Criar no Ollama" ao lado do seu modelo

### Modelos base recomendados

- `llama3.2` - Boa combina√ß√£o de velocidade e qualidade
- `phi3` - Modelo r√°pido e eficiente
- `mistral` - Excelente para tarefas em portugu√™s

## Stack T√©cnica

- **Frontend**: React + TypeScript + TailwindCSS
- **Backend**: Rust + Tauri 2.0
- **Banco de Dados**: SQLite (via rusqlite)
- **√çcones**: Lucide React

## Licen√ßa

MIT
